import requests
import base64
import os

OLLAMA_URL = "http://localhost:11434/api/generate"
DEFAULT_MODEL = "bakllava"


def send_llm_prompt(prompt: str, image_path: str = None, model: str = DEFAULT_MODEL, max_tokens: int = 600) -> str:
    """
    –í—ñ–¥–ø—Ä–∞–≤–ª—è—î —Ç–µ–∫—Å—Ç–æ–≤–∏–π –∞–±–æ –º—É–ª—å—Ç–∏–º–µ–¥—ñ–π–Ω–∏–π prompt –¥–æ –ª–æ–∫–∞–ª—å–Ω–æ—ó LLM (—á–µ—Ä–µ–∑ Ollama)

    :param prompt: –¢–µ–∫—Å—Ç–æ–≤–∏–π prompt –¥–ª—è LLM
    :param image_path: –®–ª—è—Ö –¥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
    :param model: –ù–∞–∑–≤–∞ –º–æ–¥–µ–ª—ñ Ollama (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º bakllava)
    :param max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
    :return: –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–æ–¥–µ–ª—ñ –∞–±–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø—Ä–æ –ø–æ–º–∏–ª–∫—É
    """

    data = {
        "model": model,
        "prompt": prompt,
        "stream": False,
    }

    # –î–æ–¥–∞—î–º–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —è–∫—â–æ –≤–∫–∞–∑–∞–Ω–æ
    if image_path:
        if not os.path.exists(image_path):
            return f"‚ö†Ô∏è –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {image_path}"

        try:
            with open(image_path, "rb") as f:
                img_data = base64.b64encode(f.read()).decode("utf-8")
                data["images"] = [img_data]
        except Exception as e:
            return f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —á–∏—Ç–∞–Ω–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {e}"

    try:
        response = requests.post(OLLAMA_URL, json=data)
        response.raise_for_status()
        return response.json().get("response", "").strip()

    except requests.exceptions.ConnectionError:
        return "üö´ –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—ñ–¥–∫–ª—é—á–∏—Ç–∏—Å—è –¥–æ Ollama. –ü–µ—Ä–µ–≤—ñ—Ä, —á–∏ –∑–∞–ø—É—â–µ–Ω–æ: `ollama run bakllava`"

    except requests.exceptions.HTTPError as e:
        return f"‚ùå HTTP-–ø–æ–º–∏–ª–∫–∞: {e}"

    except Exception as e:
        return f"‚ùå –ù–µ–≤—ñ–¥–æ–º–∞ –ø–æ–º–∏–ª–∫–∞: {e}"
